const canvas = document.getElementById("whiteboard");
const ctx = canvas.getContext("2d");
let drawing = false;
let erasing = false;
const penBtn = document.getElementById("penBtn");
const clearBtn = document.getElementById("clearBtn");

let currentMode = "pen";
const chatHistory = [];
x
// üñºÔ∏è Offset fix for canvas drawing
function getCanvasCoords(e) {
  const rect = canvas.getBoundingClientRect();
  return {
    x: e.clientX - rect.left,
    y: e.clientY - rect.top
  };
}

canvas.addEventListener("mousedown", () => drawing = true);
canvas.addEventListener("mouseup", () => drawing = false);
canvas.addEventListener("mouseleave", () => drawing = false);
canvas.addEventListener("mousemove", (e) => {
  if (!drawing) return;
  const { x, y } = getCanvasCoords(e);
  ctx.beginPath();
  ctx.arc(x, y, erasing ? 10 : 2, 0, Math.PI * 2);
  ctx.fillStyle = erasing ? "#ffffff" : "#000000";
  ctx.fill();
});

penBtn.addEventListener("click", () => {
  erasing = !erasing;
  penBtn.textContent = erasing ? "üñäÔ∏è Erase Mode" : "üñäÔ∏è Pen Mode";
});

clearBtn.addEventListener("click", () => {
  ctx.clearRect(0, 0, canvas.width, canvas.height);
});

const micBtn = document.getElementById("micBtn");
const chatBox = document.getElementById("chatBox");

let recognition;
if ('webkitSpeechRecognition' in window) {
  recognition = new webkitSpeechRecognition();
  recognition.continuous = false;
  recognition.interimResults = false;

  recognition.onstart = () => {
    console.log("üéôÔ∏è Listening...");
    micBtn.innerText = "üé§ Listening...";
  };

  recognition.onresult = (event) => {
    const transcript = event.results[event.results.length - 1][0].transcript.trim();
    console.log("‚úÖ Voice captured:", transcript);
    addChatMessage("You", transcript, "user");

    chatHistory.push({
      role: "user",
      parts: [{ text: transcript }]
    });

    micBtn.innerText = "‚è≥ Processing...";
    sendToGemini(transcript)
      .then(() => micBtn.innerText = "üéôÔ∏è Start Voice")
      .catch(() => micBtn.innerText = "üéôÔ∏è Start Voice");
  };

  recognition.onerror = (e) => {
    console.error("Speech recognition error:", e);
    micBtn.innerText = "üéôÔ∏è Start Voice";
  };
} else {
  alert("Speech recognition not supported in your browser.");
}

micBtn.addEventListener("click", () => {
  try {
    recognition.start();
  } catch (err) {
    console.error("Recognition start error:", err);
  }
});

function addChatMessage(sender, text, className) {
  const msg = document.createElement("div");
  msg.className = className;
  msg.innerHTML = `<b>${sender}:</b> ${text}`;
  chatBox.appendChild(msg);
  chatBox.scrollTop = chatBox.scrollHeight;
}

function speak(text) {
  window.speechSynthesis.cancel();
  const utter = new SpeechSynthesisUtterance(text);
  speechSynthesis.speak(utter);
}

function getCanvasImageBase64() {
  const tempCanvas = document.createElement("canvas");
  tempCanvas.width = canvas.width;
  tempCanvas.height = canvas.height;

  const tempCtx = tempCanvas.getContext("2d");
  tempCtx.fillStyle = "#ffffff";
  tempCtx.fillRect(0, 0, tempCanvas.width, tempCanvas.height);
  tempCtx.drawImage(canvas, 0, 0);

  return tempCanvas.toDataURL("image/png");
}

async function sendToGemini(userText) {
  const imageBase64 = getCanvasImageBase64();

  try {
    const res = await fetch("http://localhost:3000/query", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        history: chatHistory,
        image: imageBase64
      })
    });

    if (!res.ok) throw new Error("API response not OK");

    const data = await res.json();
    const botText = data.reply;

    chatHistory.push({
      role: "model",
      parts: [{ text: botText }]
    });

    addChatMessage("Bot", botText, "bot");
    speak(botText);
  } catch (err) {
    console.error("‚ùå API error:", err);
    addChatMessage("Bot", "‚ùå Oops! Failed to process your request.", "bot");
  }
}
